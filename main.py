'''
Get the sampling works for one prompt: "The movie was"

The sample sequence should looks different that the initial sequence



Issue:

1. The initial sequence is sampled by greedy, repeating sentence issue arises. Need to sample initial sequence through top-p, top-k, or beam search
2. Currently, assumes the batch size is 1. In the case when there are multiple batches, need to change the code accordingly
3. Maybe read prompts from input file?
4. Currently accepting all samples. Issue with energy function or leap frong steps
'''

import argparse
import torch
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, DistilBertForSequenceClassification, DistilBertTokenizer
import numpy as np

from sampler.SequenceEnergy import SequenceEnergy
from sampler.HMCSampler import HMCSampler


def options():
    # Initialize the ArgumentParser
    parser = argparse.ArgumentParser(description="Example script with various arguments including boolean flags and default values.")
    
    # Add argument for device (e.g., 'cpu', 'cuda')
    parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'], help='Device to run the model on (cpu or cuda) [Default: cpu]')
 
    # Add argument for lambda_energy (float) with default
    parser.add_argument('--lambda_energy', type=float, default=1.0, help='Lambda energy value [Default: 1.0]')

    # Add argument for epsilon (float) with default
    parser.add_argument('--epsilon', type=float, default=0.1, help='Epsilon value [Default: 0.1]')

    # Add argument for debug toggle (boolean flag) with default
    parser.add_argument('--debug', action='store_true', help='Enable debug mode [Default: False]')

    # Add argument for n_steps (integer) with default
    parser.add_argument('--n_steps', type=int, default=100, help='Number of steps [Default: 100]')

    # Add argument for std_dev (float) with default
    parser.add_argument('--std_dev', type=float, default=0.2, help='Standard deviation value [Default: 0.2]')

    # Add argument for delta (float) with default
    parser.add_argument('--delta', type=float, default=0.5, help='Delta value [Default: 0.5]')

    # Add argument for num_leapfrog (integer) with default
    parser.add_argument('--num_leapfrog', type=int, default=10, help='Number of leapfrog steps [Default: 10]')

    # Parse the arguments
    args = parser.parse_args()
    return args

    # Print out the values of the arguments for confirmation (or debug purposes)
    print(f"Running with the following parameters:")
    print(f"Device: {args.device}")
    print(f"Lambda Energy: {args.lambda_energy}")
    print(f"Epsilon: {args.epsilon}")
    print(f"Debug Mode: {'Enabled' if args.debug else 'Disabled'}")
    print(f"Number of Steps: {args.n_steps}")
    print(f"Standard Deviation: {args.std_dev}")
    print(f"Delta: {args.delta}")
    print(f"Number of Leapfrog Steps: {args.num_leapfrog}")

    return args

def main():
    args = options()
    device = args.device

    # GPT2 Model and Tokenizer
    model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

    # Lookup table: token IDs into token embeddings
    embed_lut = model.get_input_embeddings()
    
    # Prompt 
    prompt = "The movie was"

    # Token ids for Prompt
    prompt_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    # Token ids for tokens generated by model (Excluding prompt)
    output_ids = model.generate(prompt_ids, max_length=23, do_sample=False)[0, len(prompt_ids[0]):]
    
    # Token embeddings of output sequence (Excluding prompt)
    Y = embed_lut(output_ids).unsqueeze(0)  # [1, 20, 768]

    # Initial sentence generated by model
    decoded_text = tokenizer.decode(output_ids, skip_special_tokens=True)
    print(f"Initial sentence: {decoded_text}")

    # initialize the target sentiment embedding
    e_positive = embed_lut(tokenizer.encode("great happy good", return_tensors="pt")[0].to(device)).mean(dim=0)

    # Initialize sequence energy based on prompt and initial output token embeddings
    seq_energy = SequenceEnergy(model, prompt_ids, Y, e_positive, device=device, lambda_energy=args.lambda_energy, epsilon=args.epsilon, args)
    
    # Initialize hmc sampler
    sampler = HMCSampler(seq_energy, rng=np.random.RandomState(42), device=device, args)
    
    # sample from HMC sampler
    samples = sampler.sample(args.n_steps, args.std_dev, args.delta, args.num_leapfrog)
    print(samples[0])

    for embeddings in samples:
        print(embeddings.shape)
        scores = torch.cdist(embeddings.view(-1, embeddings.size(-1)), embed_lut.weight)
        token_ids = scores.argmin(dim=-1).view(embeddings.size(0), -1)
        token_ids = token_ids.squeeze().tolist() # Convert tensor to list for tokenizer.decode
        decoded_text = tokenizer.decode(token_ids, skip_special_tokens=True)
        print(f"sample sentence: {decoded_text}")


    

if __name__ == "__main__":
    main()
